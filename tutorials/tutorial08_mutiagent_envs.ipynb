{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 08: Multiagent Environments\n",
    "\n",
    "This tutorial covers and implementation and execution of multiagent environments in Flow. It assumes some level of knowledge or experience in writing custom environments and running experiments with RLlib; for more on these topics see `tutorial07_environments.ipynb` and `tutorial03_rllib.ipynb`, respectively. The rest of the tutorial is organized as follows. Section 1 describes the procedure through which custom environments can be augmented to generate multiagent environments. Then, section 2 walks you through an example of running a multiagent environment\n",
    "in RLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a Multiagent Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In this part we will be setting up steps to create a multiagent environment in which the agent might use \n",
    "a shared or non-shared policy. \n",
    "\n",
    "# import the base environment class\n",
    "from ray.rllib.env import MultiAgentEnv as MultiEnv\n",
    "\n",
    "# define the environment class, and inherit properties from the Multi-agent version of base env\n",
    "class MultiAgentEnv(MultiEnv, Env):\n",
    "\n",
    "# import the environment to the \n",
    "from flow.multiagent_envs.multiagent_env import MultiAgentEnv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Shared policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 None-shared policies (FIXME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in the flowong exrcise we would see the agents using 2 different policies: standard and the adverserial.\n",
    "We \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running Multiagent Environment in RLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the scenario that uses multiagent environment, \n",
    "we specify certain parameters in the flow_params. flow_param is the dictionary that is called by the \n",
    "create_env function which defines the action and bservation space for each agent during the training. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_params = dict(\n",
    "    # name of the experiment\n",
    "    exp_tag='multiagent_figure_eight',\n",
    "\n",
    "    # name of the flow environment the experiment is running on\n",
    "    env_name='MultiAgentAccelEnv',\n",
    "\n",
    "    # name of the scenario class the experiment is running on\n",
    "    scenario='Figure8Scenario',\n",
    "\n",
    "    # simulator that is used by the experiment\n",
    "    simulator='traci',\n",
    "\n",
    "    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "    sim=SumoParams(\n",
    "        sim_step=0.1,\n",
    "        render=False,\n",
    "    ),\n",
    "\n",
    "    # environment related parameters (see flow.core.params.EnvParams)\n",
    "    env=EnvParams(\n",
    "        horizon=HORIZON,\n",
    "        additional_params={\n",
    "            'target_velocity': 20,\n",
    "            'max_accel': 3,\n",
    "            'max_decel': 3,\n",
    "            'perturb_weight': 0.03,\n",
    "            'sort_vehicles': False\n",
    "        },\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Shared policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "When we run the shared policy we refer to the same policy for a different agent. In the example below, the agents\n",
    "will use'av' policy.\n",
    "\n",
    "    def gen_policy():\n",
    "        return (PPOPolicyGraph, obs_space, act_space, {})\n",
    "\n",
    "    # Setup PG with an ensemble of `num_policies` different policy graphs\n",
    "    policy_graphs = {'av': gen_policy()}\n",
    "\n",
    "    def policy_mapping_fn(_):\n",
    "        return 'av'\n",
    "\n",
    "    config.update({\n",
    "        'multiagent': {\n",
    "            'policy_graphs': policy_graphs,\n",
    "            'policy_mapping_fn': tune.function(policy_mapping_fn),\n",
    "            'policies_to_train': ['av']\n",
    "        }\n",
    "    })\n",
    "\n",
    "    return alg_run, env_name, config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 None-shared policies (FIXME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "When we run the non-shared policy we refer to different policies for each agent. The generate policy will use the \n",
    "'av' policy which would be similiar. \n",
    "    def gen_policy():\n",
    "        return (PPOPolicyGraph, obs_space, act_space, {})\n",
    "\n",
    "    # Setup PG with an ensemble of `num_policies` different policy graphs\n",
    "    policy_graphs = {'av': gen_policy(), 'adversary': gen_policy()}\n",
    "\n",
    "    def policy_mapping_fn(agent_id):\n",
    "        return agent_id\n",
    "\n",
    "    config.update({\n",
    "        'multiagent': {\n",
    "            'policy_graphs': policy_graphs,\n",
    "            'policy_mapping_fn': tune.function(policy_mapping_fn)\n",
    "        }\n",
    "    })\n",
    "    return alg_run, env_name, config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flow)",
   "language": "python",
   "name": "flow"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
