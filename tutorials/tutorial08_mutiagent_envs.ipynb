{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 08: Multiagent Environments\n",
    "\n",
    "This tutorial covers and implementation and execution of multiagent environments in Flow. It assumes some level of knowledge or experience in writing custom environments and running experiments with RLlib; for more on these topics see `tutorial07_environments.ipynb` and `tutorial03_rllib.ipynb`, respectively. The rest of the tutorial is organized as follows. Section 1 describes the procedure through which custom environments can be augmented to generate multiagent environments. Then, section 2 walks you through an example of running a multiagent environment\n",
    "in RLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a Multiagent Environment Class\n",
    "\n",
    "In this part we will be setting up steps to create a multiagent environment in which the agent might use \n",
    "a shared or non-shared policy. We begin by importing the abstract multi-agent evironment class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-24b0ac7c33b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import the base Multi-agent environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiagent_envs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiagent_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiAgentEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flow'"
     ]
    }
   ],
   "source": [
    "# import the base Multi-agent environment \n",
    "\n",
    "from flow.multiagent_envs.multiagent_env import MultiAgentEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Shared policies\n",
    "In the multi-agent environment with a shared policy, different agents will use the same policy. \n",
    "\n",
    "We define the environment class, and inherit properties from the Multi-agent version of base env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedMAEnv(MultiAgentEnv):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Env` provides the interface for running and modifying a SUMO simulation. Using this class, we are able to start sumo, provide a scenario to specify a configuration and controllers, perform simulation steps, and reset the simulation to an initial configuration.\n",
    "\n",
    "When compared to the single-agent evironment, multi-agent environment has changes in the following child classes:\n",
    "\n",
    "* **apply_rl_actions**\n",
    "* **get_state**\n",
    "* **compute_reward**\n",
    "\n",
    "Each of these components is covered in the next few subsections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the multi-agent environment we create a dictionary with Ids as keys and different parameters (acceleration, observation, etc.) as vaules for each ID.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedMAEnv(MultiAgentEnv): # update the environment class\n",
    "    def _apply_rl_actions(self, rl_actions):\n",
    "        \"\"\"Split the accelerations by ring\"\"\"\n",
    "        if rl_actions:\n",
    "            rl_ids = list(rl_actions.keys())\n",
    "            accel = list(rl_actions.values())\n",
    "            self.k.vehicle.apply_acceleration(rl_ids, accel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_state` and `compute_reward` methods support the dictionary structure of the multi-agent environment and append observation and reward, respectively, as a value for each correpsonding rl_id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedMAEnv(MultiAgentEnv): # update the environment class\n",
    "\n",
    "    def get_state(self):\n",
    "    \"\"\"See class definition.\"\"\"\n",
    "    obs = {}\n",
    "    for rl_id in self.k.vehicle.get_rl_ids():\n",
    "        lead_id = self.k.vehicle.get_leader(rl_id) or rl_id\n",
    "\n",
    "        # normalizers\n",
    "        max_speed = 15.\n",
    "        max_length = self.env_params.additional_params['ring_length'][1]\n",
    "\n",
    "        observation = np.array([\n",
    "            self.k.vehicle.get_speed(rl_id) / max_speed,\n",
    "            (self.k.vehicle.get_speed(lead_id) -\n",
    "             self.k.vehicle.get_speed(rl_id))\n",
    "            / max_speed,\n",
    "            self.k.vehicle.get_headway(rl_id) / max_length\n",
    "        ])\n",
    "        obs.update({rl_id: observation})\n",
    "\n",
    "    return obs\n",
    "    def compute_reward(self, rl_actions, **kwargs):\n",
    "        \"\"\"See class definition.\"\"\"\n",
    "        # in the warmup steps\n",
    "        if rl_actions is None:\n",
    "            return {}\n",
    "\n",
    "        rew = {}\n",
    "        for rl_id in rl_actions.keys():\n",
    "            edge_id = rl_id.split('_')[1]\n",
    "            edges = self.gen_edges(edge_id)\n",
    "            vehs_on_edge = self.k.vehicle.get_ids_by_edge(edges)\n",
    "            vel = np.array([\n",
    "                self.k.vehicle.get_speed(veh_id)\n",
    "                for veh_id in vehs_on_edge\n",
    "            ])\n",
    "            if any(vel < -100) or kwargs['fail']:\n",
    "                return 0.\n",
    "\n",
    "            target_vel = self.env_params.additional_params['target_velocity']\n",
    "            max_cost = np.array([target_vel] * len(vehs_on_edge))\n",
    "            max_cost = np.linalg.norm(max_cost)\n",
    "\n",
    "            cost = vel - target_vel\n",
    "            cost = np.linalg.norm(cost)\n",
    "\n",
    "            rew[rl_id] = max(max_cost - cost, 0) / max_cost\n",
    "        return rew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Non-shared policies (FIXME)\n",
    "\n",
    "Non-shared environment implies that the agents will be using different policies to get the reward. In the folowing exrcise we would see the agents using 'av' and the 'adverserial' policies.\n",
    "To create an environemtn for the Multi-agent non-shared policy, the following changes are to be made in:\n",
    "\n",
    "* **apply_rl_actions**\n",
    "* **get_state**\n",
    "* **compute_reward**\n",
    "\n",
    "The action_space and observation_space are the same as for the single-agent environment and can be exported from there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make `apply_rl_actions` work for multi-agent environment, we define rl_action as a combinations of each policy actions and the perturb_weight.\n",
    "In the `get_state` method, we define states for each of the policies. The adversary state and the agent state are identical\n",
    "In the `compute_reward` the agents receives opposing speed rewards. The agent receives the class definition reward and the adversary receives the negative of the agent reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultiAgentEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-826829732b61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mNonSharedMAEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiAgentEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply_rl_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrl_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0;31m# the names of all autonomous (RL) vehicles in the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             sorted_rl_ids = [\n\u001b[1;32m      5\u001b[0m                 \u001b[0mveh_id\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mveh_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorted_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MultiAgentEnv' is not defined"
     ]
    }
   ],
   "source": [
    "class NonSharedMAEnv(MultiAgentEnv):\n",
    "    def _apply_rl_actions(self, rl_actions):\n",
    "            # the names of all autonomous (RL) vehicles in the network\n",
    "            sorted_rl_ids = [\n",
    "                veh_id for veh_id in self.sorted_ids\n",
    "                if veh_id in self.k.vehicle.get_rl_ids()\n",
    "            ]\n",
    "            # define different actions for different multi-agents and calucute an rl_action \n",
    "            av_action = rl_actions['av']\n",
    "            adv_action = rl_actions['adversary']\n",
    "            perturb_weight = self.env_params.additional_params['perturb_weight']\n",
    "            rl_action = av_action + perturb_weight * adv_action\n",
    "            # use the base environment method to convert actions into accelerations for the rl vehicles\n",
    "            self.k.vehicle.apply_acceleration(sorted_rl_ids, rl_action)\n",
    "\n",
    "    def get_state(self, **kwargs):\n",
    "        state = np.array([[\n",
    "            self.k.vehicle.get_speed(veh_id) / self.k.scenario.max_speed(),\n",
    "            self.k.vehicle.get_x_by_id(veh_id) / self.k.scenario.length()\n",
    "        ] for veh_id in self.sorted_ids])\n",
    "        state = np.ndarray.flatten(state)\n",
    "        return {'av': state, 'adversary': state}\n",
    "\n",
    "    def compute_reward(self, rl_actions, **kwargs):\n",
    "        if self.env_params.evaluate:\n",
    "            reward = np.mean(self.k.vehicle.get_speed(\n",
    "                self.k.vehicle.get_ids()))\n",
    "            return {'av': reward, 'adversary': -reward}\n",
    "        else:\n",
    "            reward = rewards.desired_velocity(self, fail=kwargs['fail'])\n",
    "            return {'av': reward, 'adversary': -reward}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running Multiagent Environment in RLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the scenario that uses multiagent environment, \n",
    "we specify certain parameters in the flow_params. flow_param is the dictionary that is called by the \n",
    "create_env function which defines the action and bservation space for each agent during the training. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_params = dict(\n",
    "    # name of the experiment\n",
    "    exp_tag='multiagent_figure_eight',\n",
    "\n",
    "    # name of the flow environment the experiment is running on\n",
    "    env_name='MultiAgentAccelEnv',\n",
    "\n",
    "    # name of the scenario class the experiment is running on\n",
    "    scenario='Figure8Scenario',\n",
    "\n",
    "    # simulator that is used by the experiment\n",
    "    simulator='traci',\n",
    "\n",
    "    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "    sim=SumoParams(\n",
    "        sim_step=0.1,\n",
    "        render=False,\n",
    "    ),\n",
    "\n",
    "    # environment related parameters (see flow.core.params.EnvParams)\n",
    "    env=EnvParams(\n",
    "        horizon=HORIZON,\n",
    "        additional_params={\n",
    "            'target_velocity': 20,\n",
    "            'max_accel': 3,\n",
    "            'max_decel': 3,\n",
    "            'perturb_weight': 0.03,\n",
    "            'sort_vehicles': False\n",
    "        },\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Shared policies\n",
    "\n",
    "When we run the shared policy, we refer to the same policy for each agent. In the example below the agents\n",
    "will use 'av' policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ######################################################################\n",
    "    # Start of new code\n",
    "    ######################################################################\n",
    "    def gen_policy():\n",
    "        return (PPOPolicyGraph, obs_space, act_space, {})\n",
    "\n",
    "    # Setup PG with an ensemble of `num_policies` different policy graphs\n",
    "    policy_graphs = {'av': gen_policy()}\n",
    "\n",
    "    def policy_mapping_fn(_):\n",
    "        return 'av'\n",
    "\n",
    "    config.update({\n",
    "        'multiagent': {\n",
    "            'policy_graphs': policy_graphs,\n",
    "            'policy_mapping_fn': tune.function(policy_mapping_fn),\n",
    "            'policies_to_train': ['av']\n",
    "        }\n",
    "    })\n",
    "\n",
    "    return alg_run, env_name, config\n",
    "    ######################################################################\n",
    "    # End of new code\n",
    "    ######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 None-shared policies (FIXME)\n",
    "\n",
    "When we run the non-shared policy we refer to different policies for each agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # Start of new code\n",
    "    ######################################################################\n",
    "    \n",
    "    def gen_policy():\n",
    "        return (PPOPolicyGraph, obs_space, act_space, {})\n",
    "\n",
    "    # Setup PG with an ensemble of `num_policies` different policy graphs\n",
    "    policy_graphs = {'av': gen_policy(), 'adversary': gen_policy()}\n",
    "\n",
    "    def policy_mapping_fn(agent_id):\n",
    "        return agent_id\n",
    "\n",
    "    config.update({\n",
    "        'multiagent': {\n",
    "            'policy_graphs': policy_graphs,\n",
    "            'policy_mapping_fn': tune.function(policy_mapping_fn)\n",
    "        }\n",
    "    })\n",
    "\n",
    "    ######################################################################\n",
    "    # End of new code\n",
    "    ######################################################################\n",
    "\n",
    "    return alg_run, env_name, config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
